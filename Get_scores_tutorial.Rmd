---
title: "Analysing your forecasts"
author: "Freya Olsson"
date: "2023-04-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setting up the repository

Start by unsetting any environmental variables relating the AWS buckets to ensure we can access the right ones with the forecast scores. We will also need the `tidyverse` and `arrow` packages, which can be installed here:

```{r eval=F}
# install required packages
install.packages('arrow')
install.packages('tidyverse')

```


```{r echo = F}
Sys.unsetenv("AWS_ACCESS_KEY_ID")
Sys.unsetenv("AWS_SECRET_ACCESS_KEY")
Sys.unsetenv("AWS_DEFAULT_REGION")
Sys.unsetenv("AWS_S3_ENDPOINT")
Sys.setenv(AWS_EC2_METADATA_DISABLED="TRUE")

# load required packages
library(arrow)
library(tidyverse)
options(dplyr.summarise.inform = FALSE)
```

## Connect to the bucket

The first step is to connect to the relevent bucket in the neon4cast server. This use the `s3_bucket` function from `arrow`. We will point to the aquatics scores bucket (the other themes have their scores in a seperate bucket). 

```{r}
# connect to the scores bucket
s3 <- arrow::s3_bucket(bucket = "neon4cast-scores/parquet/aquatics", endpoint_override= "data.ecoforecast.org")

```

The object generated (`s3`), in an active binding to that bucket. From this binding you can go in and subset the data files before bringing it into your local environment. Note: this is a similar process to what you do when you use the neon4cast::stage_2() and neon4cast::stage_3() functions to get the NOAA weather forecasts (see other tutorial information).


## Subsetting and collecting the scores

You can explore what columns are in these files by using the `open_dataset` functions.

```{r}
s3 |> arrow::open_dataset()
```

Once you have decided what forecast score you want to investigate you can filter based on these columns. For example you might only want particular forecast models (`model_id`), or forecasts generated on a particular day (`reference_datetime`), or forecasts for a particular variable (`variable`), or a combination of these things. Make sure to note the column types when subsetting. For example `reference_datetime` is a string (or character) whereas datetime is a timestamp. 

We will subset the score files by `model_id`, `site_id`, `variable`, and `reference_datetime`. 

```{r}
#these are the filters you will put in
get_models <- c('persistenceRW', 'climatology') # the name of your models/the models you are interested in

start_ref_date <- as_date('2023-03-13') # what period do you want the scores for?
end_ref_date <- as_date('2023-04-13')
get_refdates <- as.character(seq(start_ref_date, end_ref_date, by = 'day'))

get_sites <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv", show_col_types = F) |> 
  dplyr::filter(field_site_subtype == 'Lake') |> # Subset to the lake sites
  select(field_site_id) |> 
  pull() # get in a vector

get_variables <- c('temperature', 'oxygen', 'chla')

```
We now have 4 vectors that we will filter the dataset by. Start by opening the dataset (`open_dataset`), then `filter` based on the above vectors. The final step is to `collect()` this into your local environment.  This process can be slow. 

We will also calculate the forecast horizon for each time step. This will be the difference between the `datetime` and the date of forecast generation or the `reference_dateime`.

```{r}
# connect to the bucket and grab the scores
scores <- open_dataset(s3) |>
  filter(reference_datetime %in% get_refdates,
         model_id %in% get_models,
         site_id %in% get_sites,
         variable %in% get_variables) |> 
  collect() |> 
# Note: this can be slow so be patient 
  # The dataframe might also be very large depending on the filters you put in above
  mutate(horizon = as_date(datetime) - as_date(reference_datetime))

```

## Visualising and summarising forecast scores

We will explore some ways you might want to investigate and visualise the performance of the forecasts. We currently have `r length(get_refdates)` individual `reference_datetimes`, over `r length(get_variable)` `variables`, `r length(get_sites)`  `site_id`s and `r length(get_models)` forecast `model_id`s. 

### 1. Comparing an inidivudal forecast for multiple models

We will try looking at temperature forecasts generated by our models for one forecast date at one site and look at how this compares with the observations. 

```{r warning=FALSE}
scores |> 
  filter(variable == 'temperature',
         reference_datetime == get_refdates[1],
         site_id == 'BARC') |> 
  ggplot(aes(x = datetime)) +
  geom_point(aes(y=observation, shape='observation')) +
  geom_ribbon(aes(ymax= quantile97.5,
                  ymin = quantile02.5,
                  colour = model_id, 
                  group = interaction(model_id,reference_datetime)),
              alpha = 0.1) +
  geom_line(aes(y = mean, 
                colour = model_id, 
                group = interaction(model_id,reference_datetime))) +
  facet_wrap(~site_id, scales = 'free') +
  
  # Make it better to look at 
  scale_colour_viridis_d(begin = 0.2, end = 0.8) +
  scale_shape(name = '') +
  labs(y= 'Water temperature (degree C)', x= 'Date', caption = 'Shaded area is the 95 % CI') +
  theme_bw()
```

These are useful for visually looking at the forecast skill but to quantitatively access we can useful one of the scoring metrics. The NEON Challenge automatically scores the forecast using the CRPS (continuous rank probability score) and the logs (ignorance) scores. These use the accuracy of the mean and the spread of the predictions. You can read more about the scores [here](https://projects.ecoforecast.org/neon4cast-docs/Evaluation.html). This is also just one forecast and it might be interesting to look at more summarised output to see how models perform over multiple forecasts. 

### 2. One day ahead forecasts over time

One summary we could look at is how a forecast of tomorrow performs across a particular time frame. We could do this by filtering the scores data frame to just ones where `horizon == 1` and plotting how the skill score changes over time. 

```{r}
scores |> 
  filter(horizon == 1, 
         variable == 'temperature') |> 
  na.omit(crps) |> 
  ggplot(aes(x=datetime, y=crps, colour = model_id)) +
  geom_line() +
  geom_point() +
  facet_wrap(~site_id)  +
  
  # Make it better to look at 
  scale_colour_viridis_d(begin = 0.2, end = 0.8) +
  labs(y= 'CRPS (degree C)', x= 'Datetime') +
  theme_bw()
```

### 3. Forecast skill over horizon

How do different model generally perform over forecast horizon? We can aggregate the forecasts to summarise the score based on how far into the future the forecast is for. For example, we could look at every 1 day ahead forecast vs every 7 day ahead forecast and see how this varies by site and variable.     

```{r}
scores |> 
  na.omit(observation) |>  # only takes datetimes with observations
  filter(variable %in% c('temperature', 'oxygen')) |> 
  group_by(horizon, variable, site_id, model_id) |> 
  summarise(crps = mean(crps)) |> 
  ggplot(aes(x = horizon, y = crps, colour = model_id)) +
  geom_line() +
  facet_grid(variable~site_id, scales = 'free') +
  
  
   # Make it better to look at 
  scale_colour_viridis_d(begin = 0.2, end = 0.8) +
  labs(y= 'CRPS (native units)', x= 'Horizon (days ahead)', caption = 'Shaded area is the 95 % CI') +
  theme_bw()
```

One thing to note is that the CRPS reports in native units (mg/L for DO and degrees celcius for temperature), and so cannot be easily compared. We can use logs scores in this case. 

```{r, echo = F}
scores |> 
  na.omit(observation) |>  # only takes datetimes with observations
  filter(variable %in% c('temperature', 'oxygen')) |> 
  group_by(horizon, variable, site_id, model_id) |> 
  summarise(logs = mean(logs)) |> 
  ggplot(aes(x = horizon, y = logs, colour = model_id)) +
  geom_line() +
  facet_grid(variable~site_id, scales = 'free')+
  
  
  # Make it better to look at 
  scale_colour_viridis_d(begin = 0.2, end = 0.8) +
  scale_shape(name = '') +
  labs(y= 'Ignorance score', x= 'Horizon (days ahead)', caption = 'Shaded area is the 95 % CI') +
  theme_bw()
```

Also these summary plots work better when there are multiple at each horizon (ie. if you have, for example, a year of forecasts). The N value on a lot of these means is small. 

