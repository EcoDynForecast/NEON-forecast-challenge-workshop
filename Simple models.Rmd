---
title: "Simple NEON forecasts"
output:
  html_document: 
    number_sections: true
  pdf_document: 
    number_sections: true
date: '2022-08-12'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Forecasting with fable {.unnumbered}

These forecasts will implement methods from the fable package which is installed via fpp3 package. Fable models require data to be in a tidy tsibble format. Tools for dealing with tsibble objects are found in the `tsibble` package. `fable` and `fabletools`, are installed as part of the `fpp3` package and deal with `mable` (model table) and `fable` (forecast table) objects. We will also use the tidyverse to manipulate and visualise the target data and forecasts. 

```{r, 'load packages', eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
library(fpp3)
library(tsibble)
library(tidyverse)
```

# Workflow
## Read in the data
Start by reading in the data from the targets file. The data are updated regularly, with the latency for the aquatics data approximately 24-48 hrs. 
Information on the sites can be found in the `NEON_Field_Site_Metadata_20220412.csv`. We filter this to look at just the aquatic sites. 
 
```{r eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
#read in the targets data
targets <- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz')

# read in the sites data
sites <- read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") |> 
  dplyr::filter(aquatics == 1)

```
```{r eval = T, echo = F}
targets[1000:1010,]
```

The columns of the targets file show the time step (daily for aquatics challenge), the 4 character site code (site_id), the variable being measured, and the mean daily observation. 

## Visualise the data
```{r eval = T, echo = F, warning=FALSE, fig.dim=c(10,10), fig.cap=c('Figure: Temperature targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Oxygen targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Chlorophyll targets data at aquatic sites provided by EFI for the NEON forecasting challgenge. Chlorophyll data is only available at lake and river sites')}
targets %>%
  filter(variable == 'temperature') %>%
  ggplot(., aes(x = time, y = observed)) +
  geom_point() +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')

targets %>%
  filter(variable == 'oxygen') %>%
  ggplot(., aes(x = time, y = observed)) +
  geom_point() +  
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')

targets %>%
  filter(variable == 'chla') %>%
  ggplot(., aes(x = time, y = observed)) +
  geom_point() +   
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')

```

## Specify the models
`fable` has some simple models that can be fitted to the target data. The models are automatically fitted to each key pairing (variable, site_id). Start with:

* the `RW` model (random walk) 
* the `SNIAVE` model that takes the value of the previous year as the estimate. Inside the model you specify `lag("1 year")`. 

### Randomwalk model
For RW forecasts, we simply set the forecast value be the value of the last observation. This is fine if we have data observed to yesterday but this often isn't the case. To make sure that the forecast uncertainty is reasonable, given the time since an observation, we start the forecast from the last observations and run forward 35 days into the future. Due to differences in past observations, the forecast will start from different dates depending on the date of the last observation.

We calculate the `start_date` and total `horizon` for each `site_id` and `variable` combination. These can then be subsetted when we run the random walk. 
```{r message=F}
# For the RW model need to start the forecast at the last non-NA day and then run to 35 days in the future
forecast_starts <- targets %>%
  filter(!is.na(observed)) %>%
  group_by(site_id, variable) %>%
  # Start the day after the most recent non-NA value
  summarise(start_date = max(time) + 1) %>% # Date 
  mutate(h = (Sys.Date() - start_date) + 35) %>% # Horizon value 
  ungroup() 

forecast_starts
```

You can see that the forecasts all have different start dates, based on when the last observation was taken.

We write a custom function that runs the RW forecast. Within this function we:

* Tidy: Takes the targets and fills with NAs, and filters up to the last non-NA value. The data must have explicit gaps for the full time series and must be in a tsibble format to run `fable`.
* Specify: Fits the RW model. We can also specify transformations to use within the model.  The fable package will automatically back-transform the forecasts whenever a transformation has been used in the model definition. 
* Forecast: Then using this model, we run a forecast. We can specify whether bootstrapping is used and the number of bootstraps.

Within this function, there are also if statements to test whether there are whole datasets missing etc. as well as messages which can be turned on/off with the `verbose = ` argument. 


```{r warning=FALSE, message =FALSE}
# Function carry out a random walk forecast
RW_daily_forecast <- function(site, var, h,
                        bootstrap = FALSE, boot_number = 200, 
                        transformation = 'none', verbose = TRUE,...) {
  
  # filter the targets data set to the site_var pair
  targets_use <- targets %>%
    dplyr::filter(site_id == site,
           variable == var) %>%
    tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'time') %>%
    # add NA values up to today (index)
    tsibble::fill_gaps(.end = Sys.Date()) %>%
    # Remove the NA's put at the end, so that the forecast starts from the last day with an observation,
    # rather than today
    dplyr::filter(time < forecast_starts$start_date[which(forecast_starts$site_id == site &
                                                            forecast_starts$variable == var)])
  
  if (nrow(targets_use) == 0) {
    message('no targets available, no forecast run')
    empty_df <- data.frame('variable' = character(),
                            'site_id' = character(),
                            '.model' = character(),
                             'time' = lubridate::ymd(),
                            '.rep' = character(),
                            '.sim' = numeric())
    
    return(empty_df)
    
  } else {
    if (transformation == 'log') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log(observed)))
    }
    if (transformation == 'log1p') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log1p(observed)))
    }
    if (transformation == 'none') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(observed))
    }
    if (transformation == 'sqrt') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(sqrt(observed)))
    }
    
    if (bootstrap == T) {
      forecast <- RW_model %>% fabletools::generate(
          h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                            forecast_starts$variable == var)]),
          bootstrap = T,
          times = boot_number
        )
    }  else
      forecast <- RW_model %>% fabletools::forecast(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                            forecast_starts$variable == var)]))
    
  if (verbose == T) {
    message(
      site,
      ' ',
      var,
      ' forecast with transformation = ',
      transformation,
      ' and bootstrap = ',
      bootstrap
    )
  }
    return(forecast)
  }
  
}

```
This function takes just one site and one variable as arguments. To run across all site_id-variable combinations we can use a for loop. We need a dataframe that we can index from.  
The number of bootstraps (`boot_number`) is set to 200 and we say that we want to log() the values - for oxygen and chlorophyll only. 

We can then loop through each variable and site and combine them into a single dataframe (`RW_forecasts`).
```{r message = F}
site_var_combinations <- expand.grid(site = unique(targets$site_id),
                                     var = unique(targets$variable)) %>% 
  # No chlorophyll forecast for wadeable streams
  filter(!(site %in% unique(sites$field_site_id)[which(sites$field_site_subtype == 'Wadeable Stream')] &
             var == 'chla')) %>%
  # assign the transformation depending on the variable. chla and oxygen get a log(x) transformation
  mutate(transformation = ifelse(var %in% c('chla', 'oxygen'), 'log', 'none')) 

for (i in 1:nrow(site_var_combinations)) {
  forecast <- RW_daily_forecast(site = site_var_combinations$site[i],
                                var = site_var_combinations$var[i],
                                boot_number = 200,
                                bootstrap = T,
                                h = 35, 
                                verbose = F,
                                transformation = site_var_combinations$transformation[i])
  
  if (!exists('RW_forecast')) {
    RW_forecast <- forecast
  } else {
    RW_forecast <- bind_rows(RW_forecast, forecast)
  }
  # print(paste0("forecast ", i, '/', nrow(site_var_combinations)))
}

```

This produces a forecast table or `fable`, which has columns for `variable`, `site_id`, the `.model`, the bootstrap value (`.rep`), and the prediction (`.sim`). 
```{r}
RW_forecast %>%
  filter(site_id == 'SUGG')
```


```{r, message = F, warning = F, echo = F, fig.cap = "Figure: Example 'random walk' forecasts for Lake Suggs (FL)"}
RW_forecast %>% 
  filter(site_id == 'SUGG') %>%
  ggplot(.,aes(x=time, y=.sim, group = .rep)) + geom_line(alpha = 0.4) + 
  geom_line(data = subset(targets, site_id == 'SUGG'),
            aes(x=time, y=observed, group = 'obs'), colour = 'black') +
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() + theme(legend.position = 'none') +
  coord_cartesian(xlim = c(min(forecast_starts$start_date[which(forecast_starts$site_id == 'SUGG')]) - 10,
                           Sys.Date() + 35)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed')
```

Each line on the plot is one of the ensemble members (shown in the fable as `.rep`). You can also see that not all the "forecasted" days are true forecasts (some are in the past), but we started the forecast at the last observation. Therefore when we write out the forecast and submit it we need to make sure to only submit the true forecast. 

### Convert to EFI standard
For an ensemble forecast the documentation specifies the following columns:

* `time`: forecast timestamp
* `start_time`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of start_time in the file
* `site_id`: NEON code for site
* `ensemble`: integer value for forecast replicate (i.e. `.rep`);
* `variable`: standardized variable name from the theme 
* `predicted`: forecasted value (from `.sim`)

The challenge also requires 'true' forecasts only (i.e predictions of future observations) so we filter for times in the future. 
```{r}
RW_forecasts_EFI <- RW_forecast %>%
  rename(ensemble = .rep,
         predicted = .sim) %>%
  # For the EFI challenge we only want the forecast for future
  filter(time > Sys.Date()) %>%
  group_by(site_id, variable) %>%
  mutate(start_time = min(time) - lubridate::days(1)) %>%
  select(time, start_time, site_id, ensemble, variable, predicted) 
```

Now we have a forecast that can be submitted to the EFI challenge.

```{r}
RW_forecasts_EFI %>%
  filter(site_id == 'SUGG')
```

## Seasonal naive model
An alternative approach might be to look at the historic data to make predictions about the future.The seasonal naive model sets each forecast to be equal to the last observed value from the same season (e.g., the same day of the previous year). 
Again we need to tidy the data to the correct format for `fable`. We make sure there are explicit gaps (using `fill_gaps()`) and make it into a tsibble with `variable` and `site_id` as the keys and `time` as the index. 
Then the `SNAIVE` model is fit with a 1 year lag. 

```{r, warning = F}
SN_model <- targets %>%
  as_tsibble(key = c('variable', 'site_id'), index = 'time') %>%
  # add NA values up to today (index)
  fill_gaps(.end = Sys.Date()) %>%
  model(SN = SNAIVE(observed ~ lag('1 year')))
```

Use the model we've specified to forecast. `h=35` specifies the horizon of the forecast, relative to the index of the data as 35 days. If the index, in this case `time`, had a different value such as monthly, the h = value would be months. We use `forecast()` rather than `generate()` for the non-bootstrapped version. The forecast will run for each key combination (variable-site_id). 

```{r, warning=FALSE, error=FALSE, message=FALSE}
simple_SN <- SN_model %>% forecast(h = 35, bootstrap = F)
simple_SN
```
The output from this function is a `fable`. The predicted are held in the `observed` column as an S3 distribution, which gives the mean and variance of the prediction. `N()` says that the distribution is normal. 

We can calculate the standard deviation of the predicted values using a function to extract the variance and mean and create a table in the right format for EFI.
This function extracts the variance from the distribution and calculates the standard deviation (sigma). 
The columns needed for a distributional forecast are:

* `time`: forecast timestamp
* `start_time`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of start_time in the file
* `site_id`: NEON code for site
* `family`: name of probability distribution that is described by the parameter values in the parameter column; only normal is currently allowed
* `parameter`: required to be the string mu (mean) or sigma (standard deviation)
* `variable`: standardized variable name from the theme
* `predicted`: forecasted value for parameter in the parameter column

```{r}
convert.to.efi_standard <- function(df){
  ## determine variable name
  var <- attributes(df)$dist
  ## Normal distribution: use distribution mean and variance
  df %>% 
    dplyr::mutate(sigma = sqrt( distributional::variance( .data[[var]] ) ) ) %>%
    dplyr::rename(mu = .mean) %>%
    dplyr::select(time, site_id, .model, mu, sigma) %>%
    tidyr::pivot_longer(c(mu, sigma), names_to = "parameter", values_to = var) %>%
    dplyr::rename('predicted' = var) %>%
    mutate(family = "normal",
           start_time = min(time) - lubridate::days(1)) %>%
    select(any_of(c('time', 'start_time', 'site_id', 'family', 'parameter', 'variable', 'predicted')))
}

```

```{r}
simple_SN_efi <- convert.to.efi_standard(simple_SN)
```
```{r echo = FALSE}
simple_SN_efi %>%
  filter(site_id == 'SUGG')
```

This is now in the right format to be submitted to the NEON4casting challenge. 

```{r, message = F, warning = F, echo = F, fig.cap="Figure: Example 'seasonal naive' forecasts for Lake Suggs (FL). Shade area show 95% confidence intervals"}
simple_SN_efi %>% 
  filter(site_id == 'SUGG') %>%
  pivot_wider(names_from = 'parameter', values_from = 'predicted') %>%
  ggplot(.,aes(x=time)) + 
  geom_ribbon(aes(ymax = mu + (1.96*sigma),
                  ymin = mu - (1.96*sigma)), alpha = 0.2, fill = 'blue') +
  geom_line(aes(y = mu)) + 
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() +
  coord_cartesian(xlim = c(min(forecast_starts$start_date[which(forecast_starts$site_id == 'SUGG')]) - 30,
                           Sys.Date() + 35)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed') +
  geom_line(data = subset(targets, site_id == 'SUGG'), aes(x=time, y= observed))
```
You can see that there are gaps in the forecast. This is where there was no observation for that day in the previous year. Ways we might fill these gaps in the forecast include interpolating the values, taking a day-of-year mean for all years data. 

### Write the forecast for NEON EFI challenge

## Generate metadata
Using the `neon4cast::generate_metadata()` function to write the metadata for each forecast.

Start by making the team list:
```{r}
team_list <- list(list(individualName = list(givenName = "Freya", 
                                             surName = "Olsson"),
                       organizationName = "Virginia Tech",
                       electronicMailAddress = "freyao@vt.edu"),
                  list(individualName = list(givenName = "Quinn", 
                                             surName = "Thomas"),
                       organizationName = "Virginia Tech"))
```

Then information about the model goes in the model metadata list:

```{r eval = F}
model_metadata = list(
  forecast = list(
    model_description = list(
      forecast_model_id = 'naive_baseline',  # model identifier:
      name = 'Simple naive forecast using persistence',#Name or short description of model
      type = 'empirical', #General type of model empirical, machine learning, process
      repository =  # put your GitHub Repository in here
    ),
    initial_conditions = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = , #How many models states need initial conditions; delete if status = absent
     #Delete list below if status = absent, present, or data_driven
      propagation = list(
        type =  , #How does your model propogate initial conditions ('ensemble' is most common)
        size =  #number of ensemble members
      ),
      #Delete list below UNLESS status = assimilates
      assimilation = list(
        type = , #description of assimilation method
        reference = , #reference for assimilation method
        complexity =  #number of states that are updated with assimilation
      )
    ),
    drivers = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = , #How many drivers are used? Delete if status = absent
      #Delete list below if status = absent, present, or data_driven
      propagation = list( 
        type = , #How does your model propogate driver (ensemble or MCMC is most common
        size = #number of ensemble or MCMC members
        ) 
    ),
    parameters = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = , #How many parameters are included?; Delete if status = absent
      #Delete list below below blank if status = absent, present, or data_driven
      propagation = list(
        type = , #how does your model propogate parameter uncertainity?
        size = ),
      #Delete list below UNLESS status = assimilates  
      assimilation = list(
        type = , #description of assimilation method
        reference = , #reference for assimilation method
        complexity =  #number of states that are updated with assimilation
      )
    ),
    random_effects = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = ,  #Delete if status = absent
      #Delete list below if status = absent, present, or data_driven
      propagation = list(
        type = , #How does your model propogate random effects (ensemble or MCMC is most common)
        size =  #number of ensemble or MCMC members
      ),
      #Delete list below NLESS status = assimilates
      assimilation = list(
        type = , #description of assimilation method
        reference = , #reference for assimilation method
        complexity =  #number of states that are updated with assimilation
      )
    ),
    process_error = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = , #Delete if status = absent
      #Delete the list below below blank if status = absent, present, or data_driven
      propagation = list(
        type = , #How does your model propagate random effects uncertainty (ensemble or MCMC is most common) 
        size =  #How many ensemble or MCMC members
      ),
      #Delete the list below UNLESS status = assimilates
      assimilation = list(
        type = , #Name of data assilimilation method
        reference = , #Reference for data assimilation method
        complexity = , #Number of states assimilate
        covariance = , #TRUE OR FALSE
        localization =  #TRUE OR FALSE
      )
    ),
    obs_error = list(
      status = , #options: absent, present, data_driven, propagates, assimilates
      complexity = , #Delete if status = absent
      #Delete the list below below blank if status = absent, present, or data_driven
      propagation = list(
        type = , #How does your model propagate random effects uncertainty (ensemble or MCMC is most common) 
        size =  #How many ensemble or MCMC members
      )
    )
  )
)
```


```{r eval = FALSE}
neon4cast::generate_metadata(forecast_file = file.path('C:/Users/freya/Documents/VT 2022-/neon4casting challenge/NEON-simple-baselines/aquatics-', 
                                                       Sys.Date(), 
                                                       '-naive_forecast.csv'),
                             team_list = team_list,
                             model_metadata =  model_metadata,
                             forecast_iteration_id = ,
                             title = ,
                             team_name = ,
                             intellectualRights = 
                             ) 
```

## Submit forecast 
Files need to be in the correct format with metadata for submission
```{r eval = FALSE}
neon4cast::submit(forecast_file = file.path("aquatics-forecast-", Sys.Date(), "-naive_forecast.csv"),
                  metadata = file.path("aquatics-forecast-", Sys.Date(), "-naive_forecast.xml"))
```

## Alternative methods to loop through each variable-site_id combination
Using the `purrr` package we can also loop through each combination of site_id and variable combination. 
This is more effecient computationally than the for loop. You need to create a dataframe with each argument as a column. Then specify this, along with the RW function as arguments in `pmap_dfr`. The `dfr` part of the function specifies that the output should be use row_bind into a dataframe. 

```{r message = F}
site_var_combinations <- 
  # Gets every combination of site_id and variable
  expand.grid(site = unique(targets$site_id),
              var = unique(targets$variable)) %>%
  # assign the transformation depending on the variable.
  mutate(transformation = 'none') %>%
  mutate(boot_number = 200,
         h = 35,
         bootstrap = T, 
         verbose = T)

# Runs the RW forecast for each combination of variable and site_id
RW_forecasts <- purrr::pmap_dfr(site_var_combinations, RW_daily_forecast) 

```



