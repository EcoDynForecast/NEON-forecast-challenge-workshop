---
title: "Simple NEON forecasts"
output:
  html_document: 
    number_sections: true
  pdf_document: 
    number_sections: true
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Forecasting with fable {.unnumbered}

These forecasts will implement methods from the fable package which is installed via fpp3 package. Fable models require data to be in a tidy tsibble format. Tools for dealing with tsibble objects are found in the `tsibble` package. `fable` and `fabletools`, are installed as part of the `fpp3` package and deal with `mable` (model table) and `fable` (forecast table) objects. We will also use the tidyverse to manipulate and visualise the target data and forecasts. 

```{r, 'load packages', eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
library(fpp3)
library(tsibble)
library(tidyverse)
options(dplyr.summarise.inform = FALSE)
```

# Workflow
## Read in the data
Start by reading in the data from the targets file. The data are updated regularly, with the latency for the aquatics data approximately 24-48 hrs. 
Information on the sites can be found in the `NEON_Field_Site_Metadata_20220412.csv`. We filter this to look at just the aquatic sites. 
 
```{r eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
#read in the targets data
targets <- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz')

# read in the sites data
sites <- read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") |> 
  dplyr::filter(aquatics == 1)

```
```{r eval = T, echo = F}
targets[1000:1010,]
```

The columns of the targets file show the time step (daily for aquatics challenge), the 4 character site code (site_id), the variable being measured, and the mean daily observation. 

## Visualise the data
```{r eval = T, echo = F, warning=FALSE, fig.dim=c(10,10), fig.cap=c('Figure: Temperature targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Oxygen targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Chlorophyll targets data at aquatic sites provided by EFI for the NEON forecasting challgenge. Chlorophyll data is only available at lake and river sites')}
targets %>%
  filter(variable == 'temperature') %>%
  ggplot(., aes(x = datetime, y = observed)) +
  geom_point() +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y') +
  labs(title = 'temperature')

targets %>%
  filter(variable == 'oxygen') %>%
  ggplot(., aes(x = datetime, y = observed)) +
  geom_point() +  
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')+
  labs(title = 'oxygen')

targets %>%
  filter(variable == 'chla') %>%
  ggplot(., aes(x = datetime, y = observed)) +
  geom_point() +   
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')+
  labs(title = 'chla')

```

## Specify the models
`fable` has some simple models that can be fitted to the target data. The models are automatically fitted to each key pairing (variable, site_id). Start with:

* the `RW` model (random walk) 
* the `SNIAVE` model that takes the value of the previous year as the estimate. Inside the model you specify `lag("1 year")`. 

### Randomwalk model
For RW forecasts, we simply set the forecast value be the value of the last observation. This is fine if we have data observed to yesterday but this often isn't the case. To make sure that the forecast uncertainty is reasonable, given the time since an observation, we start the forecast from the last observations and run forward 35 days into the future. Due to differences in past observations, the forecast will start from different dates depending on the date of the last observation.

We calculate the `start_date` and total `horizon` for each `site_id` and `variable` combination. These can then be subsetted when we run the random walk. 
```{r message=F}
# For the RW model need to start the forecast at the last non-NA day and then run to 35 days in the future
forecast_starts <- targets %>%
  filter(!is.na(observed)) %>%
  group_by(site_id, variable) %>%
  # Start the day after the most recent non-NA value
  summarise(start_date = max(datetime) + 1) %>% # Date 
  mutate(h = (Sys.Date() - start_date) + 35) %>% # Horizon value 
  ungroup() 

forecast_starts
```

You can see that the forecasts all have different start dates, based on when the last observation was taken.

We write a custom function that runs the RW forecast. Within this function we:

* Tidy: Takes the targets and fills with NAs, and filters up to the last non-NA value. The data must have explicit gaps for the full time series and must be in a tsibble format to run `fable`.
* Specify: Fits the RW model. We can also specify transformations to use within the model.  The fable package will automatically back-transform the forecasts whenever a transformation has been used in the model definition. 
* Forecast: Then using this model, we run a forecast. We can specify whether bootstrapping is used and the number of bootstraps.

Within this function, there are also if statements to test whether there are whole datasets missing etc. as well as messages which can be turned on/off with the `verbose = ` argument. 


```{r warning=FALSE, message =FALSE}
# Function carry out a random walk forecast
RW_daily_forecast <- function(site, var, h,
                        bootstrap = FALSE, boot_number = 200, 
                        transformation = 'none', verbose = TRUE,...) {
  message('starting ',site_var_combinations$site[i], ' forecast')
  # filter the targets data set to the site_var pair
  targets_use <- targets %>%
    dplyr::filter(site_id == site,
           variable == var) %>%
    tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
    # add NA values up to today (index)
    tsibble::fill_gaps(.end = Sys.Date()) %>%
    # Remove the NA's put at the end, so that the forecast starts from the last day with an observation,
    # rather than today
    dplyr::filter(datetime < forecast_starts$start_date[which(forecast_starts$site_id == site &
                                                                forecast_starts$variable == var)])
  
  if (nrow(targets_use) == 0) {
    message('no targets available, no forecast run')
    empty_df <- data.frame('variable' = character(),
                            'site_id' = character(),
                            '.model' = character(),
                             'time' = lubridate::ymd(),
                            '.rep' = character(),
                            '.sim' = numeric())
    
    return(empty_df)
    
  } else {
    if (transformation == 'log') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log(observed)))
    }
    if (transformation == 'log1p') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(log1p(observed)))
    }
    if (transformation == 'none') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(observed))
    }
    if (transformation == 'sqrt') {
      RW_model <- targets_use %>%
        fabletools::model(RW = fable::RW(sqrt(observed)))
    }
    
    if (bootstrap == T) {
      forecast <- RW_model %>% 
        fabletools::generate(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                                      forecast_starts$variable == var)]),
                             bootstrap = T,
                             times = boot_number)
    }  else
      forecast <- RW_model %>% 
        fabletools::forecast(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                                      forecast_starts$variable == var)]))
    
  if (verbose == T) {
    message(
      site,
      ' ',
      var,
      ' forecast with transformation = ',
      transformation,
      ' and bootstrap = ',
      bootstrap
    )
  }
    return(forecast)
    
  }
  
}

```
This function takes just one site and one variable as arguments. To run across all site_id-variable combinations we can use a for loop. We need a dataframe that we can index from.  
The number of bootstraps (`boot_number`) is set to 200 and we say that we want to log() the values - for oxygen and chlorophyll only. 

We can then loop through each variable and site and combine them into a single dataframe (`RW_forecasts`).
```{r message = F}
site_var_combinations <- expand.grid(site = unique(targets$site_id),
                                     var = unique(targets$variable)) %>% 
  # No chlorophyll forecast for wadeable streams
  filter(!(site %in% unique(sites$field_site_id)[which(sites$field_site_subtype == 'Wadeable Stream')] &
             var == 'chla')) %>%
  # assign the transformation depending on the variable. chla and oxygen get a log(x) transformation
  mutate(transformation = ifelse(var %in% c('chla', 'oxygen'), 'log', 'none')) 

RW_forecast <- NULL

for (i in 1:nrow(site_var_combinations)) {
  forecast <- RW_daily_forecast(site = site_var_combinations$site[i],
                                var = site_var_combinations$var[i],
                                boot_number = 200,
                                bootstrap = T,
                                h = 35, 
                                verbose = F,
                                transformation = site_var_combinations$transformation[i])
  
  
  RW_forecast <- bind_rows(RW_forecast, forecast)
  
}

```

This produces a forecast table or `fable`, which has columns for `variable`, `site_id`, the `.model`, the bootstrap value (`.rep`), and the prediction (`.sim`). 
```{r}
RW_forecast %>%
  filter(site_id == 'SUGG')
```


```{r, message = F, warning = F, echo = F, fig.cap = "Figure: Example 'random walk' forecasts for Lake Suggs (FL)"}
RW_forecast %>% 
  filter(site_id == 'SUGG') %>%
  ggplot(.,aes(x=datetime, y=.sim, group = .rep)) + geom_line(alpha = 0.4) + 
  geom_point(data = subset(targets, site_id == 'SUGG'),
            aes(x=datetime, y=observed, group = 'obs'), colour = 'black') +
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() + theme(legend.position = 'none') +
  coord_cartesian(xlim = c(min(forecast_starts$start_date[which(forecast_starts$site_id == 'SUGG')]) - 10,
                           Sys.Date() + 35)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed')
```

Each line on the plot is one of the ensemble members (shown in the fable as `.rep`). You can also see that not all the "forecasted" days are true forecasts (some are in the past), but we started the forecast at the last observation. Therefore when we write out the forecast and submit it we need to make sure to only submit the true forecast. 

### Convert to EFI standard
For an ensemble forecast the documentation specifies the following columns:

* `time`: forecast timestamp
* `reference_datetime`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of reference_datetime in the file
* `site_id`: NEON code for site
* `parameter`: integer value for forecast replicate (i.e. `.rep`);
* `variable`: standardized variable name from the theme 
* `predicted`: forecasted value (from `.sim`)

The challenge also requires 'true' forecasts only (i.e predictions of future observations) so we filter for times in the future. 
```{r}
RW_forecasts_EFI <- RW_forecast %>%
  rename(parameter = .rep,
         predicted = .sim) %>%
  # For the EFI challenge we only want the forecast for future
  filter(datetime > Sys.Date()) %>%
  group_by(site_id, variable) %>%
  mutate(reference_datetime = min(datetime) - lubridate::days(1),
         family = 'ensemble',
         model_id = 'GLEON_RW') %>%
  select(model_id, datetime, reference_datetime, site_id, family, parameter, variable, predicted) 
```

Now we have a forecast that can be submitted to the EFI challenge.

```{r}
RW_forecasts_EFI %>%
  filter(site_id == 'SUGG')
```

## Seasonal naive model
An alternative approach might be to look at the historic data to make predictions about the future. The seasonal naive model in `fable` sets each forecast to be equal to the last observed value from the same season (e.g., the same day of the previous year). 
Again we need to tidy the data to the correct format for `fable`. We make sure there are explicit gaps (using `fill_gaps()`) and make it into a tsibble with `variable` and `site_id` as the keys and `time` as the index. 
Then the `SNAIVE` model is fit with a 1 year lag. 

```{r, warning = F}
SN_model <- targets %>%
  as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
  # add NA values up to today (index)
  fill_gaps(.end = Sys.Date()) %>%
  model(SN = SNAIVE(observed ~ lag('1 year')))
```

Use the model we've specified to forecast. `h=35` specifies the horizon of the forecast, relative to the index of the data as 35 days. If the index, in this case `time`, had a different value such as monthly, the h = value would be months. We use `forecast()` rather than `generate()` for the non-bootstrapped version. The forecast will run for each key combination (variable-site_id). 

```{r, warning=FALSE, error=FALSE, message=FALSE}
simple_SN <- SN_model %>% forecast(h = 35, bootstrap = F)
simple_SN
```
The output from this function is a `fable`. The predicted are held in the `observed` column as an S3 distribution, which gives the mean and variance of the prediction. `N()` says that the distribution is normal. 

We can calculate the standard deviation of the predicted values using a function to extract the variance and mean and create a table in the right format for EFI.
This function extracts the variance from the distribution and calculates the standard deviation (sigma). 
The columns needed for a distributional forecast are:

* `datetime`: forecast timestamp
* `reference_datetime`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of reference_datetime in the file
* `site_id`: NEON code for site
* `family`: name of probability distribution that is described by the parameter values in the parameter column; only normal or ensemble is currently allowed
* `parameter`: required to be the string mu (mean) or sigma (standard deviation) or the ensemble number
* `variable`: standardized variable name from the theme
* `predicted`: forecasted value for parameter in the parameter column

```{r}
convert.to.efi_standard <- function(df){
  ## determine variable name
  var <- attributes(df)$dist
  ## Normal distribution: use distribution mean and variance
  df %>% 
    dplyr::mutate(sigma = sqrt( distributional::variance( .data[[var]] ) ) ) %>%
    dplyr::rename(mu = .mean) %>%
    dplyr::select(datetime, site_id, .model, mu, sigma) %>%
    tidyr::pivot_longer(c(mu, sigma), names_to = "parameter", values_to = var) %>%
    dplyr::rename('predicted' = var) %>%
    mutate(family = "normal",
           reference_datetime = min(datetime) - lubridate::days(1),
           model_id = 'GLEON_SN') %>%
    select(any_of(c('model_id', 'datetime', 'reference_datetime', 'site_id', 'family', 'parameter', 'variable', 'predicted')))
}

```

```{r}
simple_SN_efi <- convert.to.efi_standard(simple_SN)
```
```{r echo = FALSE}
simple_SN_efi %>%
  filter(site_id == 'SUGG')
```

This is now in the right format to be submitted to the NEON4casting challenge. 

```{r, message = F, warning = F, echo = F, fig.cap="Figure: Example 'seasonal naive' forecasts for Lake Suggs (FL). Shade area show 95% confidence intervals"}
simple_SN_efi %>% 
  filter(site_id == 'SUGG') %>%
  pivot_wider(names_from = 'parameter', values_from = 'predicted') %>%
  ggplot(.,aes(x=datetime)) + 
  geom_ribbon(aes(ymax = mu + (1.96*sigma),
                  ymin = mu - (1.96*sigma)), alpha = 0.2, fill = 'blue') +
  geom_line(aes(y = mu)) + 
  facet_wrap(~variable, scales = 'free') + 
  theme_bw() +
  coord_cartesian(xlim = c(min(forecast_starts$start_date[which(forecast_starts$site_id == 'SUGG')]) - 30,
                           Sys.Date() + 35)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed') +
  geom_point(data = subset(targets, site_id == 'SUGG'), aes(x=datetime, y= observed))
```
You can see that there are gaps in the forecast. This is where there was no observation for that day in the previous year. Ways we might fill these gaps in the forecast include interpolating the values, taking a day-of-year mean for all years data. 

### Write the forecast for NEON EFI challenge
```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- Sys.Date()
forecast_name <- 'GLEON_RW_forecast.csv'
forecast_file <- paste(theme, date, forecast_name, sep = '-')

write_csv(RW_forecasts_EFI, forecast_file)

```
## Submit forecast 
Files need to be in the correct format with metadata for submission. The forecast organisers have created tools to help aid in the submission process. These tools can be downloaded from Github using `devtools::install_github(eco4cast/neon4cast)`.
These include functions for submitting, scoring and reading forecasts:

* `submit()` - submit the forecast file to the neon4cast server where it will be scored
* `forecast_output_validator()` - will check the file is in the correct format to be submitted
* `check_submission()` - check that your submission has been uploaded to the server
* `score`

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = forecast_file,
                  ask = F) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

## Alternative methods to loop through each variable-site_id combination
Using the `purrr` package we can also loop through each combination of site_id and variable combination. 
This is more efficient computationally than the for loop. You need to create a dataframe with each argument as a column. Then specify this, along with the RW function as arguments in `pmap_dfr`. The `dfr` part of the function specifies that the output should be use row_bind into a dataframe. 

```{r message = F}
site_var_combinations <- 
  # Gets every combination of site_id and variable
  expand.grid(site = unique(targets$site_id),
              var = unique(targets$variable)) %>%
  # assign the transformation depending on the variable.
  mutate(transformation = 'none') %>%
  mutate(boot_number = 200,
         h = 35,
         bootstrap = T, 
         verbose = T)

# Runs the RW forecast for each combination of variable and site_id
RW_forecasts <- purrr::pmap_dfr(site_var_combinations, RW_daily_forecast) 

```


# Introducing co-variates

One important step to creating forecasts is to introduce co-variates to the model. A water temperature forecast, for example, may be improved by additional information about past and future weather. The neon4cast challenge package includes functions for downloading past and future NOAA weather forecasts for all of the NEON sites. The 3 types of data are as follows:

* historic weather: combination of day 1 NOAA weather forecasts (i.e., when the forecasts are most accurate) to generate an estimate of past weather. You can download this “stacked” NOAA product using `neon4cast::noaa_stage3()`
* future weather: challenge organizers subset the NOAA weather forecasts which can be downloaded using `neon4cast::noaa_stage2()`. Future weather forecasts include a 30-member ensemble of equally likely future weather conditions

The weather data can be subsetted and filtered prior to download (requires `dplyr::collect()`).


```{r, message=FALSE}
# past stacked weather
df_past <- neon4cast::noaa_stage3()
noaa_past <- df_past |> 
  dplyr::filter(site_id %in% sites$field_site_id,
                time >= ymd('2017-01-01'),
                variable == "air_temperature") |> 
  dplyr::collect()

noaa_past
```

This is a stacked ensemble forecast of the one day ahead forecasts. To get an estimate of the historic conditions we can take a mean of these ensembles. We will also need to convert the temperatures to celcius from Kelvin.

```{r}
# aggregate the past to mean values
noaa_past_mean <- noaa_past |> 
  mutate(datetime = as_date(time)) |> 
  group_by(datetime, site_id) |> 
  summarize(air_temperature = mean(predicted, na.rm = TRUE), .groups = "drop") |> 
  # convert air temp to C
  mutate(air_temperature = air_temperature - 273.15)
```

We can then look at the future weather forecasts in the same way but using the `noaa_stage2()`. The forecast becomes available from NOAA at 5am UTC the following day, so we take the air temperature forecast from yesterday to make the water quality forecasts. Then we can use the ensembles to produce uncertainty in the water quality forecast by grouping the forecasts by site and ensemble when aggregating.


```{r, message=FALSE}
# New forecast only available at 5am UTC the next day
forecast_date <- as.character(Sys.Date() - 1)

df_future <- neon4cast::noaa_stage2()

noaa_future <- df_future |> 
  dplyr::filter(cycle == 0,
                start_date == forecast_date,
                site_id %in% sites$field_site_id,
                variable == "air_temperature") |> 
  dplyr::collect()

```

The forecasts are hourly and we are interested in using daily mean weather in the forecast generation for water temperature.

```{r warning=F}
noaa_future_daily <- noaa_future |> 
  mutate(datetime = as_date(time)) |> 
  # mean daily forecasts at each site per ensemble
  group_by(datetime, site_id, ensemble) |> 
  summarize(air_temperature = mean(predicted)) |>
  # convert to Celsius
  mutate(air_temperature = air_temperature - 273.15) |> 
  select(datetime, site_id, air_temperature, ensemble)

noaa_future_daily
```

We will fit a simple linear model between historic air temperature and the water temperature targets data. Using this model we can then use our future estimates of air temperature (all 30 ensembles) to estimate water temperature at each site. The ensemble weather forecast will therefore propagate uncertainty into the water temperature forecast and give an estimate of driving data uncertainty. 

We will start by joining the historic weather data with the targets to aid in fitting the linear model.

```{r}
targets_lm <- targets |> 
  filter(variable == 'temperature') |>
  pivot_wider(names_from = 'variable', values_from = 'observed') |> 
  left_join(noaa_past_mean, 
            by = c("datetime","site_id"))
targets_lm
```

To fit the linear model we use the base R `lm()` but there are also methods to fit linear (and non-linear) models in the `fable::` package. See here for more information on the `fable::TSLM()` function. We can fit a seperate linear model for each site. For example, at Lake Suggs, this would look like:

```{r, eval = F}
example_site <- 'SUGG'

site_target <- targets_lm |> 
  filter(site_id == example_site)

noaa_future_site <- noaa_future_daily |> 
  filter(site_id == example_site)

#Fit linear model based on past data: water temperature = m * air temperature + b
fit <- lm(site_target$temperature ~ site_target$air_temperature)
    
# use linear regression to forecast water temperature for each ensemble member
forecasted_temperature <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature

```
We can loop through this for each site to create a site-wise forecast of water temperature based on a linear model and each forecasted air temperature. We can run this forecast for each site and then bind them together to submit as one forecast. We can add in an if else statement for sites where there is no data collect.

```{r}
temp_lm_forecast <- NULL

for(i in 1:length(sites$field_site_id)) {  
  example_site <- sites$field_site_id[i]
  
  site_target <- targets_lm |>
    filter(site_id == example_site)

  noaa_future_site <- noaa_future_daily |> 
    filter(site_id == example_site)
  
  # some sites have no temperature records so need to be skipped over
  if(length(which(!is.na(site_target$air_temperature) & !is.na(site_target$temperature))) > 0){
    
    #Fit linear model based on past data: water temperature = m * air temperature + b
    fit <- lm(site_target$temperature ~ site_target$air_temperature)
    
    # use linear regression to forecast water temperature for each ensemble member
    forecasted_temperature <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature
    
    # put all the relavent information into a tibble that we can bind together
    temperature <- tibble(datetime = noaa_future_site$datetime,
                          site_id = example_site,
                          ensemble = noaa_future_site$ensemble,
                          predicted = forecasted_temperature,
                          variable = "temperature")
    
    temp_lm_forecast <- dplyr::bind_rows(temp_lm_forecast, temperature)
    # message(example_site, ' temperature forecast run')
  } else {
    message('no forecast run at ', example_site)
  }
}
```
We now have 30 possible forecasts of water temperature at each site (except BLUE). On this plot each line represents one of the possible forecasts and the range of forecast a quantification of the uncertainty in our forecast. 

```{r, echo = F}
temp_lm_forecast %>%
  filter(site_id == 'SUGG' | site_id == 'TOOK') %>%
  ggplot(., aes(x=datetime, y=predicted, group = ensemble)) +
  geom_line() +
  facet_wrap(~site_id)
```

We need to make sure the dataframe is in the correct format and then we can submit this to the challenge as well!

```{r}
temp_lm_forecast_EFI <- temp_lm_forecast %>%
  mutate(model_id = 'GLEON_lm',
         reference_datetime = Sys.Date(),
         family = 'ensemble',
         ensemble = as.character(ensemble)) %>%
  rename(parameter = 'ensemble')%>%
  select(model_id, datetime, reference_datetime, site_id, family, parameter, variable, predicted)
```

```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- Sys.Date()
forecast_name <- 'GLEON_LM_forecast.csv'
forecast_file <- paste(theme, date, forecast_name, sep = '-')

write_csv(temp_lm_forecast_EFI, forecast_file)

```

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = forecast_file,
                  ask = F) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

# Other things that might be useful
## Other weather variables 
You can look at what other variables are available in the NOAA weather data

```{r}
df_past %>%
  filter(site_id == 'ARIK',
         time > ymd('2022-01-01')) |> 
  dplyr::collect() |> 
  distinct(variable)
```

## 
